{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "The data is downloaded from https://www.kaggle.com/datasets/ltcmdrdata/plain-text-wikipedia-202011?resource=download\n",
    "\n",
    "Before we can used it to train vectors, we need to do some pre-processing. Among others throwing away things that we do not need.\n",
    "\n",
    "And a tokenization process that makes sure that we do not have out of vocabulary terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File management\n",
    "\n",
    "Because the size of the wikipedia data is quite big (8GB when zipped) I store it on an external hard disk. For this, some file management must be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the folder where the wiki dump is located.\n",
    "\n",
    "wikidump_folder = \"/media/hugo/Seagate Expansion Drive/wiki_dump\"\n",
    "if not (os.path.exists(wikidump_folder)):\n",
    "    raise Exception(f\"Folder {wikidump_folder} does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the filename of the wiki dump\n",
    "\n",
    "wikidump_filename = f\"{wikidump_folder}/wikidump_2020_11.zip\"\n",
    "if not (os.path.exists(wikidump_filename)):\n",
    "    raise Exception(f\"File {wikidump_filename} does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605\n"
     ]
    }
   ],
   "source": [
    "# Show number of files in wikidump zip file\n",
    "import zipfile\n",
    "with zipfile.ZipFile(wikidump_filename, 'r') as zip_ref:\n",
    "    print(len(zip_ref.namelist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_texts_filename = f\"{wikidump_folder}/wiki_texts.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and tokenization\n",
    "\n",
    "First preprocess the file. Remove\n",
    "* digits\n",
    "* punctuation\n",
    "* stop words\n",
    "\n",
    "Also all text will be lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = text.split()\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    text = \" \".join(filtered_sentence)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tokenization I use the wordpiece tokenizer. See https://huggingface.co/course/chapter6/6?fw=pt for a good resource about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize(text:str) -> list:\n",
    "    return tokenizer.encode(text).tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the preprocessing\n",
    "\n",
    "The following functions apply the preprocessing to all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available cores: 4\n",
      "Using 3 cores\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622d08ba3c3a4059975cb42e9c8683d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing json files:   0%|          | 0/605 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import hashlib\n",
    "from multiprocessing import Pool\n",
    "# import sentence tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# get available cores\n",
    "cores = os.cpu_count()\n",
    "print(f\"Available cores: {cores}\")\n",
    "use_cores = cores - 1\n",
    "# use_cores = cores\n",
    "print(f\"Using {use_cores} cores\")\n",
    "\n",
    "# current nice value\n",
    "current_nice = os.nice(0)\n",
    "\n",
    "# lower priority\n",
    "os.nice(0)\n",
    "\n",
    "preprocessed_text_folder = f\"{wikidump_folder}/preprocessed_texts\"\n",
    "\n",
    "if not (os.path.exists(preprocessed_text_folder)):\n",
    "    os.mkdir(preprocessed_text_folder)\n",
    "\n",
    "# open the first file in the wiki dump zip file\n",
    "\n",
    "def process_text(text):\n",
    "    preprocessed_text = preprocess(text)\n",
    "    tokens = tokenize(preprocessed_text)[1:-1]\n",
    "    tokenized_text = \" \".join(tokens)\n",
    "    return tokenized_text\n",
    "\n",
    "\n",
    "# def save_preprocessed_text(text, zip_out):\n",
    "#     # get hash of text\n",
    "#     text_hash = hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "#     text_filename = f\"{text_hash}.txt\"\n",
    "\n",
    "#     # save text to zip file\n",
    "\n",
    "\n",
    "def process_json_file(json_filename, zip_ref):\n",
    "    with zip_ref.open(json_filename) as f:\n",
    "        items = json.loads(f.read())\n",
    "        # for item in items:\n",
    "        #     text = item[\"text\"]\n",
    "        #     preprocessed_text = preprocess(text)\n",
    "\n",
    "        # preprocessed_texts = [preprocess(item[\"text\"]) for item in tqdm(items, desc=\"Preprocessing texts\")]\n",
    "\n",
    "        # texts = [item[\"text\"] for item in items]\n",
    "        texts = [sent_tokenize( item[\"text\"]) for item in items]\n",
    "        # print(len(texts))\n",
    "        # flatten list\n",
    "        texts = [item for sublist in texts for item in sublist]\n",
    "        # print(len(texts))\n",
    "\n",
    "        with Pool(use_cores) as myPool:\n",
    "            preprocessed_texts = myPool.map(process_text, texts)\n",
    "        # print(preprocessed_texts[0:10])\n",
    "        # get hash of preprocessed texts\n",
    "        preprocessed_texts_string = json.dumps(preprocessed_texts)\n",
    "        preprocessed_texts_hash = hashlib.md5(preprocessed_texts_string.encode()).hexdigest()\n",
    "\n",
    "        preprocessed_texts_filename = f\"{preprocessed_text_folder}/{preprocessed_texts_hash}.json\"\n",
    "        if not (os.path.exists(preprocessed_texts_filename)):\n",
    "            json.dump(preprocessed_texts, open(preprocessed_texts_filename, 'w'))\n",
    "\n",
    "        # print(f\"Saved preprocessed texts to {preprocessed_texts_filename}\")\n",
    "        assert os.path.exists(preprocessed_texts_filename)\n",
    "        # print(tokenized_text)\n",
    "\n",
    "\n",
    "def process_archive(filename):\n",
    "    with zipfile.ZipFile(wikidump_filename, 'r') as zip_ref:\n",
    "        json_filenames = zip_ref.namelist()\n",
    "        for i, json_filename in tqdm(enumerate(json_filenames), total=len(json_filenames), desc=\"Processing json files\"):\n",
    "            # print(i,json_filename)\n",
    "            # print(f\"Processing {json_filename}\")\n",
    "            process_json_file(json_filename, zip_ref)\n",
    "\n",
    "try:\n",
    "    process_archive(wikidump_filename)\n",
    "finally:\n",
    "    # set nice value back to original value\n",
    "    os.nice(current_nice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zipping\n",
    "\n",
    "Evert json file is about 30 MB big. Given that we have 600 files, this is about 18 GB.\n",
    "In order to save space, I archive everything to a zip and remove the original files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/hugo/Seagate Expansion Drive/wiki_dump/preprocessed_texts.zip'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip the preprocessed texts folder\n",
    "\n",
    "preprocessed_zipfile = f\"{wikidump_folder}/preprocessed_texts\"\n",
    "\n",
    "import shutil\n",
    "shutil.make_archive(preprocessed_zipfile, 'zip', preprocessed_text_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When zipped it is about **X** gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove all json files from preprocessed texts folder\n",
    "\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# files = glob.glob(f\"{preprocessed_text_folder}/*.json\")\n",
    "# for f in files:\n",
    "#     os.remove(f)\n",
    "#     assert not os.path.exists(f), f\"File {f} still exists!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection\n",
    "\n",
    "Because of the limitations of my computer, I want my corpus to have a maximum size of **X** gb when unzipped. So I have to make a selection. I will just make a random selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc48b7f97eeefcfa973ac84946cdeb32dcd8538d584fc23cdbd11e050afa8c03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
